---
title: "IDS investigation worksheet"
author: "by YDTeam: Cecilia, Frank, Junkai, Zhandos"
date: "`r Sys.Date()`"
output: html_document
---

**Note:** You can use this file as you 'working document' where you can try out various investigation ideas and keep notes about your findings. How you use and structure this file is up to you. It is recommended that you keep notes about what you are investigating and what you find as this will make the process of creating your presentation and report easier. Please note that you _do not_ need to submit this file as part of your group project.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load-lib, message = FALSE}
library(tidyverse)
library(readr)

adjudications <- read_csv("data/CSV/AdjudicationsQ12023.csv")




```


```{r load-data}
library(tidyverse)
library(janitor)
 
adjudications <- adjudications %>%
  clean_names()

adj_clean <- adjudications %>%
  drop_na()


adj_clean <- adj_clean %>%
  rename(quarter = date)


glimpse(adj_clean)
colSums(is.na(adj_clean))

load("./data/CSV/punishments.rdata")

pun_clean <- punishments %>%
  rename(
    sex = Sex,
    age_group = `Age group`,
    no_days_raw = `No of days`
  ) %>%
  mutate(
   
    no_days_raw = ifelse(no_days_raw == ".", NA, no_days_raw),
    no_days = as.numeric(no_days_raw),
    sex = factor(sex),
    age_group = factor(age_group, ordered = TRUE)
  ) %>%
  filter(
    !is.na(sex),
    !is.na(age_group),           
    !is.na(no_days),
    no_days > 0
  )


```

```{r}
library(dplyr)
library(ggplot2)


adj_clean %>%
  filter(sex == "M") %>% 
  group_by(offence) %>%
  summarise(total = sum(count, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(total)) %>%
  slice_head(n = 6) %>%
  ggplot(aes(x = reorder(offence, total), y = total, fill = offence)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Top 6 Offences for Males (Q1 2023)",
    x = "Offence",
    y = "Total cases"
  )

adj_clean %>%
  filter(sex == "F") %>% 
  group_by(offence) %>%
  summarise(total = sum(count, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(total)) %>%
  slice_head(n = 6) %>%
  ggplot(aes(x = reorder(offence, total), y = total, fill = offence)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Top 6 Offences for Females (Q1 2023)",
    x = "Offence",
    y = "Total cases"
  )

```


```{r}
# check how many people are in each age group

adj_clean %>%
  filter(age_group == "15 - 17") %>% 
  summarise(count = n())

age_offence_counts <- adj_clean %>% 
  group_by(age_group) %>% 
  count(age_group)
age_offence_counts

```
```{r}
# find 
```

```{r, include=FALSE, message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(forcats)
library(scales)
library(gt)
library(glue)
library(janitor)

adj <- read_csv("AdjudicationsQ12023.csv") |> clean_names()

adj_clean <- adj |>
  mutate(
    count = suppressWarnings(as.numeric(count)),
    age_group = as.character(age_group),
    offence   = as.character(offence)
  ) |>
  filter(!is.na(age_group), !is.na(offence), !is.na(count), count > 0)

age_order <- c("15 - 17","18 - 20","21 - 24","25 - 29","30 - 39",
               "40 - 49","50 - 59","60 - 69","70 and over")
adj_clean <- adj_clean |>
  mutate(age_group = fct_relevel(age_group, age_order, after = 0))

age_offence_counts <- adj_clean |>
  group_by(age_group, offence) |>
  summarise(total = sum(count), .groups = "drop")

age_totals <- age_offence_counts |>
  group_by(age_group) |>
  summarise(age_total = sum(total), .groups = "drop")

age_offence_share <- age_offence_counts |>
  left_join(age_totals, by = "age_group") |>
  mutate(pct = total / age_total)

p_counts <- age_offence_counts |>
  ggplot(aes(x = total, y = age_group, fill = offence)) +
  geom_col() +
  geom_text(
    data = age_totals,
    aes(x = age_total, y = age_group, label = comma(age_total)),
    nudge_x = max(age_totals$age_total, na.rm = TRUE) * 0.02, 
    inherit.aes = FALSE, size = 3
  ) +
  labs(
    title = "Adjudications by Age Group and Offence (Q1 2023)",
    x = "Total Cases",
    y = "Age Group",
    fill = "Offence"
  ) +
  theme_minimal(base_size = 12)

p_share_labels <- age_offence_share |>
  mutate(lbl = ifelse(pct >= 0.05, percent(pct, accuracy = 1), "")) |>
  ggplot(aes(x = pct, y = age_group, fill = offence)) +
  geom_col() +
  geom_text(
    aes(label = lbl),
    position = position_stack(vjust = 0.5),
    size = 3, color = "white"
  ) +
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Composition of Offence Types within Each Age Group (Q1 2023)",
    x = "Share within Age Group",
    y = "Age Group",
    fill = "Offence"
  ) +
  theme_minimal(base_size = 12)
```

```{r}
library(tidymodels)

pun_clean <- pun_clean %>% 
  mutate(
    age_midpoint = (strtoi(substr(age_group, 1, 2)) + strtoi(substr(age_group, 6, 7))) / 2
      ) %>% 
  drop_na()

library(ggplot2)
ggplot(data = pun_clean, aes(x = age_midpoint, y = Sum)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE)


pun_clean_fit <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(Sum ~ age_midpoint, data = pun_clean)

pun_clean_fit_aug <- augment(pun_clean_fit$fit)

ggplot(pun_clean_fit_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "darkgrey", lty = "dashed")


severe_fit <- severe_wf %>% fit(data = data_train)

tidy(severe_fit)

#Predict probabilities on the testing dataset
severe_pred <- 
  predict(severe_fit, data_test, type = "prob") %>%
  bind_cols(data_test %>% select(severe))


#create ROC diagram and calculate the area
roc_severe <- 
  roc_curve(severe_pred, truth = severe, .pred_severe)

autoplot(roc_severe)

auc_severe <- 
  roc_auc(severe_pred, truth = severe, .pred_severe)

auc_severe


```

```{r}
library(tidymodels)
library(ggplot2)

pun_clean <- pun_clean %>% 
  mutate(
    age_midpoint = (strtoi(substr(age_group, 1, 2)) + strtoi(substr(age_group, 6, 7))) / 2
      ) %>% 
  drop_na()

poly_rec <- recipe(Sum ~ age_midpoint, data = pun_clean) %>%
  step_poly(age, degree = 2)

poly_model

```

```{r}
adj_clean %>%
  group_by(detailed_offence) %>%
  summarise(total = sum(count, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(total)) %>%
  slice_head(n = 20)
```

# severe offences: Possession of unauthorised articles (???), Threats / Abusive and Insulting racist words/behaviour, Cause damage to any part of a prison, Drug related offence, Fights with any person, Endangers health/safety of any person, Assault on a prisoner, Assault on staff, Commits any assault	, Obstructs an officer executing his duty, Attempts: Assault on staff, Assault on any other person


```{r}
library(tidymodels)


# Convert offence categories into a binary classification variable (severe vs not_severe)
severe_levels <- c("Possession of unauthorised articles", "Threats / Abusive and Insulting racist words/behaviour", "Cause damage to any part of a prison, Drug related offence", "Fights with any person", "Endangers health/safety of any person", "Assault on a prisoner", "Assault on staff", "Commits any assault", "Obstructs an officer executing his duty", "Attempts: Assault on staff", "Assault on any other person")

adj_clean <- adj_clean %>% 
  mutate(
    severe = if_else(detailed_offence %in% severe_levels,
                     "severe", "not_severe"),
    severe = factor(severe, levels = c("not_severe", "severe"))
    )

adj_clean <- adj_clean %>% 
  mutate(
    age_midpoint = (strtoi(substr(age_group, 1, 2)) + strtoi(substr(age_group, 6, 7))) / 2
      ) %>% 
  drop_na()


# Fix random numbers by setting the seed 
# Enables analysis to be reproducible when random numbers are used 
set.seed(123)

# Put 80% of the data into the training set 
data_split <- initial_split(adj_clean, prop = 0.8, strata = severe)
# Create data frames for the two sets:
data_train <- training(data_split)
data_test  <- testing(data_split)


#Fit a model to the training dataset
log_spec <- logistic_reg() %>%
  set_engine("glm") 

severe_rec <- recipe(
  severe ~ age_midpoint , # formula
  data = data_train) %>%                           # data to use for cataloging names and types of variables
  step_unknown(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors()) %>%   #Create dummy variables
  step_zv(all_predictors())        #Remove zero variance variables

summary(severe_rec)


severe_wf <- 
  workflow() %>%
  add_recipe(severe_rec) %>%
  add_model(log_spec)


severe_fit <- severe_wf %>% fit(data = data_train)

tidy(severe_fit) %>% print()

#Predict probabilities on the testing dataset
severe_pred <- predict(severe_fit, data_test, type = "prob") %>%
  bind_cols(data_test)


#create ROC diagram and calculate the area
severe_pred %>%  
  roc_curve(truth = severe, .pred_severe, event_level = "second") %>% 
  autoplot()

auc_severe <- 
  roc_auc(severe_pred, truth = severe, .pred_severe, event_level = "second")

auc_severe


```


```{r}
library(yardstick)
library(ggplot2)

# 生成分类预测（分类结果而不是概率）
severe_pred_class <- predict(severe_fit, data_test) %>%
  bind_cols(data_test %>% select(severe))

# 混淆矩阵
conf_mat(severe_pred_class, truth = severe, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_fill_gradient(low = "#a5d8ff", high = "#1e90ff") +
  labs(title = "Confusion Matrix for Logistic Regression",
       x = "Truth",
       y = "Prediction") +
  theme_minimal()



# 计算 sensitivity（查全率 / 召回率）
sensitivity(severe_pred_class, truth = severe, estimate = .pred_class)

# 计算 specificity（查准率）
specificity(severe_pred_class, truth = severe, estimate = .pred_class)


```